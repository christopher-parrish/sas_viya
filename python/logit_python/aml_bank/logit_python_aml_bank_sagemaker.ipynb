{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes to install SAS Python packages\n",
    "###### In SageMaker, open terminal in Jupyter/VS code\n",
    "###### install numactl at prompt: sudo apt-get install -y numactl\n",
    "###### pip install swat\n",
    "###### pip install sasctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Grid node action status report: 1 nodes, 9 total actions executed.\n",
      "[About]\n",
      "\n",
      " {'CAS': 'Cloud Analytic Services',\n",
      "  'CASCacheLocation': 'CAS Disk Cache',\n",
      "  'CASHostAccountRequired': 'OPTIONAL',\n",
      "  'Copyright': 'Copyright Â© 2014-2024 SAS Institute Inc. All Rights Reserved.',\n",
      "  'ServerTime': '2024-07-25T18:53:24Z',\n",
      "  'System': {'Hostname': 'controller.sas-cas-server-default.innovationlab.svc.cluster.local',\n",
      "   'Linux Distribution': 'Red Hat Enterprise Linux release 8.10 (Ootpa)',\n",
      "   'Model Number': 'x86_64',\n",
      "   'OS Family': 'LIN X64',\n",
      "   'OS Name': 'Linux',\n",
      "   'OS Release': '5.15.0-1064-azure',\n",
      "   'OS Version': '#73-Ubuntu SMP Tue Apr 30 14:24:24 UTC 2024'},\n",
      "  'Transferred': 'NO',\n",
      "  'Version': '4.00',\n",
      "  'VersionLong': 'V.04.00M0P06172024',\n",
      "  'Viya Release': '20240717.1721252687211',\n",
      "  'Viya Version': 'Stable 2024.06',\n",
      "  'license': {'expires': '21Sep2024:00:00:00',\n",
      "   'gracePeriod': 0,\n",
      "   'site': 'CIS SSEMONTHLY INNOVATION ENTERPRISE-RISK-MRM',\n",
      "   'siteNum': 70180938,\n",
      "   'warningPeriod': 15}}\n",
      "\n",
      "[nodestatus]\n",
      "\n",
      " Node Status\n",
      " \n",
      "                                                 name        role  uptime  running  stalled\n",
      " 0  controller.sas-cas-server-default.innovationla...  controller   2.509        0        0\n",
      "\n",
      "[server]\n",
      "\n",
      " Server Status\n",
      " \n",
      "    nodes  actions\n",
      " 0      1        9\n",
      "\n",
      "+ Elapsed: 0.000538s, user: 0.000384s, sys: 0.000117s, mem: 0.305mb\n"
     ]
    }
   ],
   "source": [
    "### delete in SageMaker - firewall is preventing connection ###\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "filepath = input(\"file path to credentials: \")\n",
    "sys.path.append(filepath)\n",
    "from credentials import hostname, session, port, protocol, wd, output_dir, git_dir, token_dir, token, token_refresh, token_pem\n",
    "\n",
    "import swat\n",
    "\n",
    "access_token = open(token, \"r\").read()\n",
    "conn =  swat.CAS(hostname=hostname, username=None, password=access_token, ssl_ca_list=token_pem, protocol=protocol)\n",
    "print(conn.serverstatus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id                     float64\n",
      "num_transactions               float64\n",
      "credit_score                   float64\n",
      "marital_status_single          float64\n",
      "marital_status_married         float64\n",
      "marital_status_divorced        float64\n",
      "analytic_partition             float64\n",
      "ml_indicator                   float64\n",
      "checking_only_indicator        float64\n",
      "prior_ctr_indicator            float64\n",
      "address_change_2x_indicator    float64\n",
      "cross_border_trx_indicator     float64\n",
      "in_person_contact_indicator    float64\n",
      "linkedin_indicator             float64\n",
      "atm_deposit_indicator          float64\n",
      "trx_10ksum_indicator           float64\n",
      "common_merchant_indicator      float64\n",
      "direct_deposit_indicator       float64\n",
      "citizenship_country_risk       float64\n",
      "occupation_risk                float64\n",
      "num_acctbal_chgs_gt2000        float64\n",
      "distance_to_employer           float64\n",
      "distance_to_bank               float64\n",
      "income                         float64\n",
      "primary_transfer_cash          float64\n",
      "primary_transfer_check         float64\n",
      "primary_transfer_wire          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### delete in SageMaker - firewall is preventing connection ###\n",
    "\n",
    "### caslib and table to use in modeling\n",
    "caslib = 'casuser'\n",
    "in_mem_tbl = 'AML_BANK_PREP'\n",
    "\n",
    "### load table in-memory if not already exists in-memory\n",
    "if conn.table.tableExists(caslib=caslib, name=in_mem_tbl).exists<=0:\n",
    "    conn.table.loadTable(caslib=caslib, path=str(in_mem_tbl+str('.sashdat')), \n",
    "                         casout={'name':in_mem_tbl, 'caslib':caslib, 'promote':True})\n",
    "\n",
    "### show table to verify\n",
    "conn.table.tableInfo(caslib=caslib, wildIgnore=False, name=in_mem_tbl)\n",
    "\n",
    "dm_inputdf =  conn.CASTable(in_mem_tbl, caslib=caslib).to_frame()\n",
    "\n",
    "### print columns for review of model parameters\n",
    "print(dm_inputdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id                       int64\n",
      "num_transactions               float64\n",
      "credit_score                   float64\n",
      "marital_status_single            int64\n",
      "marital_status_married           int64\n",
      "marital_status_divorced          int64\n",
      "analytic_partition               int64\n",
      "ml_indicator                     int64\n",
      "checking_only_indicator          int64\n",
      "prior_ctr_indicator              int64\n",
      "address_change_2x_indicator      int64\n",
      "cross_border_trx_indicator       int64\n",
      "in_person_contact_indicator      int64\n",
      "linkedin_indicator               int64\n",
      "atm_deposit_indicator            int64\n",
      "trx_10ksum_indicator             int64\n",
      "common_merchant_indicator        int64\n",
      "direct_deposit_indicator         int64\n",
      "citizenship_country_risk         int64\n",
      "occupation_risk                  int64\n",
      "num_acctbal_chgs_gt2000        float64\n",
      "distance_to_employer           float64\n",
      "distance_to_bank               float64\n",
      "income                         float64\n",
      "primary_transfer_cash            int64\n",
      "primary_transfer_check           int64\n",
      "primary_transfer_wire            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### Create Dataframe ###\n",
    "########################\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "workspace_dir = \"/home/sagemaker-user\"\n",
    "data_table = \"aml_bank_prep.csv\"\n",
    "\n",
    "dm_inputdf = pd.read_csv(Path(workspace_dir) / data_table, header=0)\n",
    "print(dm_inputdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### Model Parameters ###\n",
    "########################\n",
    "\n",
    "### import python libraries\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "### model manager information\n",
    "metadata_output_dir = 'outputs'\n",
    "model_name = 'logit_python_aml_bank_sagemaker'\n",
    "project_name = 'Anti-Money Laundering'\n",
    "description = 'Logistic Regression'\n",
    "model_type = 'logistic_regression'\n",
    "model_function = 'Classification'\n",
    "predict_syntax = 'predict_proba'\n",
    "username = 'sagemaker_user'\n",
    "table_name = 'aml_bank_prep'\n",
    "\n",
    "### define macro variables for model\n",
    "dm_dec_target = 'ml_indicator'\n",
    "dm_partitionvar = 'analytic_partition'\n",
    "create_new_partition = 'no' # 'yes', 'no'\n",
    "dm_key = 'account_id' \n",
    "dm_classtarget_level = ['0', '1']\n",
    "prediction_labels = ['P_ml_indicator0', 'P_ml_indicator1', 'I_ml_indicator']\n",
    "event_prob_var = ['1', '0']\n",
    "dm_partition_validate_val, dm_partition_train_val, dm_partition_test_val = [0, 1, 2]\n",
    "dm_partition_validate_perc, dm_partition_train_perc, dm_partition_test_perc = [0.3, 0.6, 0.1]\n",
    "\n",
    "### create list of regressors\n",
    "keep_predictors = [\n",
    "    'marital_status_single',\n",
    "    'checking_only_indicator',\n",
    "    'prior_ctr_indicator',\n",
    "    'address_change_2x_indicator',\n",
    "    'cross_border_trx_indicator',\n",
    "    'in_person_contact_indicator',\n",
    "    'linkedin_indicator',\n",
    "    'citizenship_country_risk',\n",
    "    'distance_to_employer',\n",
    "    'distance_to_bank'\n",
    "    ]\n",
    "#rejected_predictors = []\n",
    "\n",
    "### mlflow\n",
    "use_mlflow = 'no' # 'yes', 'no'\n",
    "mlflow_run_to_use = 0\n",
    "mlflow_class_labels =['TENSOR']\n",
    "mlflow_predict_syntax = 'predict'\n",
    "\n",
    "### var to consider in bias assessment\n",
    "bias_vars = ['marital_status_single']\n",
    "\n",
    "### var to consider in partial dependency\n",
    "pd_var1 = 'distance_to_employer'\n",
    "pd_var2 = 'distance_to_bank'\n",
    "\n",
    "### create partition column, if not already in dataset\n",
    "if create_new_partition == 'yes':\n",
    "    dm_inputdf = shuffle(dm_inputdf)\n",
    "    dm_inputdf.reset_index(inplace=True, drop=True)\n",
    "    validate_rows = round(len(dm_inputdf)*dm_partition_validate_perc)\n",
    "    train_rows = round(len(dm_inputdf)*dm_partition_train_perc) + validate_rows\n",
    "    test_rows = len(dm_inputdf)-train_rows\n",
    "    dm_inputdf.loc[0:validate_rows,dm_partitionvar] = dm_partition_validate_val\n",
    "    dm_inputdf.loc[validate_rows:train_rows,dm_partitionvar] = dm_partition_train_val\n",
    "    dm_inputdf.loc[train_rows:,dm_partitionvar] = dm_partition_test_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marital_status_single', 'checking_only_indicator', 'prior_ctr_indicator', 'address_change_2x_indicator', 'cross_border_trx_indicator', 'in_person_contact_indicator', 'linkedin_indicator', 'citizenship_country_risk', 'distance_to_employer', 'distance_to_bank']\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Final Modeling Columns ###\n",
    "##############################\n",
    "\n",
    "### create list of model variables\n",
    "dm_input = list(dm_inputdf.columns.values)\n",
    "macro_vars = (dm_dec_target + ' ' + dm_partitionvar + ' ' + dm_key).split()\n",
    "rejected_predictors = [i for i in dm_input if i not in keep_predictors]\n",
    "rejected_vars = rejected_predictors # + macro_vars (include macro_vars if rejected_predictors are explicitly listed - not contra keep_predictors)\n",
    "for i in rejected_vars:\n",
    "    dm_input.remove(i)\n",
    "print(dm_input)\n",
    "\n",
    "### create prediction variables\n",
    "dm_predictionvar = [str('P_') + dm_dec_target + dm_classtarget_level[0], str('P_') + dm_dec_target + dm_classtarget_level[1]]\n",
    "dm_classtarget_intovar = str('I_') + dm_dec_target\n",
    "\n",
    "##################\n",
    "### Data Split ###\n",
    "##################\n",
    "\n",
    "### create train, test, validate datasets using existing partition column\n",
    "dm_traindf = dm_inputdf[dm_inputdf[dm_partitionvar] == dm_partition_train_val]\n",
    "X_train = dm_traindf.loc[:, dm_input]\n",
    "y_train = dm_traindf[dm_dec_target]\n",
    "dm_testdf = dm_inputdf.loc[(dm_inputdf[dm_partitionvar] == dm_partition_test_val)]\n",
    "X_test = dm_testdf.loc[:, dm_input]\n",
    "y_test = dm_testdf[dm_dec_target]\n",
    "dm_validdf = dm_inputdf.loc[(dm_inputdf[dm_partitionvar] == dm_partition_validate_val)]\n",
    "X_valid = dm_validdf.loc[:, dm_input]\n",
    "y_valid = dm_validdf[dm_dec_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_train: 0.9660878685467894\n",
      "score_test: 0.9615384615384616\n",
      "score_valid: 0.9638778839431368\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Training Code - Python ###\n",
    "##############################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### estimate & fit model\n",
    "dm_model = LogisticRegression(\n",
    "        tol=1e-8,\n",
    "        fit_intercept=True,\n",
    "        solver='newton-cg',\n",
    "        verbose=0,\n",
    "        max_iter=100\n",
    "    )\n",
    "dm_model.fit(X_train, y_train)\n",
    "\n",
    "print('score_train:', dm_model.score(X_train, y_train))\n",
    "print('score_test:', dm_model.score(X_test, y_test))\n",
    "print('score_valid:', dm_model.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Logistic Regression\n",
      "model_parameters\n",
      "LogisticRegression(solver='newton-cg', tol=1e-08)\n",
      " \n",
      "model_performance\n",
      "score_full: 0.9649699342749266\n",
      "score_train: 0.9660878685467894\n",
      "score_test: 0.9615384615384616\n",
      "score_valid: 0.9638778839431368\n",
      "confusion_matrix:\n",
      "(tn, fp, fn, tp)\n",
      "(1345, 15, 40, 30)\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98      1360\n",
      "         1.0       0.67      0.43      0.52        70\n",
      "\n",
      "    accuracy                           0.96      1430\n",
      "   macro avg       0.82      0.71      0.75      1430\n",
      "weighted avg       0.96      0.96      0.96      1430\n",
      "\n",
      "intercept:\n",
      "[-6.40563549]\n",
      "odds_ratios:\n",
      "                    predictors odds_ratio\n",
      "0        marital_status_single   9.539825\n",
      "1      checking_only_indicator   3.631731\n",
      "2          prior_ctr_indicator   2.966805\n",
      "3  address_change_2x_indicator   5.393386\n",
      "4   cross_border_trx_indicator   5.111662\n",
      "5  in_person_contact_indicator   0.140842\n",
      "6           linkedin_indicator   0.128186\n",
      "7     citizenship_country_risk    1.52751\n",
      "8         distance_to_employer   1.065238\n",
      "9             distance_to_bank   1.010887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "### score full data\n",
    "fullX = dm_inputdf.loc[:, dm_input]\n",
    "fully = dm_inputdf[dm_dec_target]\n",
    "#plot_roc_curve(dm_model, fullX, fully)\n",
    "dm_scoreddf_prob = pd.DataFrame(dm_model.predict_proba(fullX), columns=dm_predictionvar)\n",
    "dm_scoreddf_class = pd.DataFrame(dm_model.predict(fullX), columns=[dm_classtarget_intovar])\n",
    "columns_actual = bias_vars + [dm_dec_target]\n",
    "dm_scoreddf_bias = pd.DataFrame(dm_inputdf, columns=columns_actual)\n",
    "dm_scoreddf = pd.concat([dm_scoreddf_prob, dm_scoreddf_class], axis=1)\n",
    "scored = pd.concat([dm_scoreddf, dm_scoreddf_bias], axis=1)\n",
    "\n",
    "### create tables with predicted values\n",
    "trainProba = dm_model.predict_proba(X_train)\n",
    "trainProbaLabel = dm_model.predict(X_train)\n",
    "testProba = dm_model.predict_proba(X_test)\n",
    "testProbaLabel = dm_model.predict(X_test)\n",
    "validProba = dm_model.predict_proba(X_valid)\n",
    "validProbaLabel = dm_model.predict(X_valid)\n",
    "trainData = pd.concat([y_train.reset_index(drop=True), pd.Series(data=trainProbaLabel), pd.Series(data=trainProba[:,1])], axis=1)\n",
    "testData = pd.concat([y_test.reset_index(drop=True), pd.Series(data=testProbaLabel), pd.Series(data=testProba[:,1])], axis=1)\n",
    "validData = pd.concat([y_valid.reset_index(drop=True), pd.Series(data=validProbaLabel), pd.Series(data=validProba[:,1])], axis=1)\n",
    "trainData.columns = ['actual', 'predict', 'probability']\n",
    "testData.columns = ['actual', 'predict', 'probability']\n",
    "validData.columns = ['actual', 'predict', 'probability']\n",
    "\n",
    "### print model & results\n",
    "predictions = dm_model.predict(X_test)\n",
    "cols = X_train.columns\n",
    "predictors = np.array(cols)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(description)\n",
    "print(description)\n",
    "print('model_parameters')\n",
    "print(dm_model)\n",
    "print(' ')\n",
    "print('model_performance')\n",
    "print('score_full:', dm_model.score(fullX, fully))\n",
    "print('score_train:', dm_model.score(X_train, y_train))\n",
    "print('score_test:', dm_model.score(X_test, y_test))\n",
    "print('score_valid:', dm_model.score(X_valid, y_valid))\n",
    "print('confusion_matrix:')\n",
    "print('(tn, fp, fn, tp)')\n",
    "print((tn, fp, fn, tp))\n",
    "print('classification_report:')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "### print logit odds ratios\n",
    "orat = np.exp(dm_model.coef_, out=None)\n",
    "c1 = np.vstack([predictors,orat])\n",
    "c2 = np.transpose(c1)\n",
    "c = pd.DataFrame(c2, columns=['predictors', 'odds_ratio'])\n",
    "print('intercept:')\n",
    "print(dm_model.intercept_)\n",
    "print('odds_ratios:')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "### Create Model Artifacts using sasctl package ###\n",
    "###      Register Model in Model Manager        ###\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# target_df = y_train\n",
    "# predictors = np.array(X_train.columns)\n",
    "# target_event = dm_classtarget_level[1]\n",
    "# num_target_categories = len(dm_classtarget_level)\n",
    "# predict_method = str('{}.')+str(predict_syntax)+str('({})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chparr\\\\OneDrive - SAS\\\\python\\\\outputs\\\\logit_python_aml_bank_sagemaker\\\\logit_python_aml_bank_sagemaker.ipynb'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### delete in SageMaker ###\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "### create directories for metadata\n",
    "output_path = Path(output_dir) / metadata_output_dir / model_name\n",
    "if output_path.exists() and output_path.is_dir():\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "### create output path\n",
    "os.makedirs(output_path)\n",
    "output_path\n",
    "\n",
    "### create python requirements file\n",
    "requirements = [\n",
    "    {\n",
    "        \"step\":\"import math, pickle, pandas as pd, numpy as np, settings\",\n",
    "        \"command\":\"pip3 install math==3.10.5 pickle==3.10.5 numpy==1.20.3 pandas==1.3.4 settings==0.2.2\"\n",
    "    }\n",
    "]\n",
    "requirementsObj = json.dumps(requirements, indent = 4)\n",
    "with open(str(output_path)+str('/requirements.json'), 'w') as outfile:\n",
    "    outfile.write(requirementsObj)\n",
    "    \n",
    "### copy .py script to output path\n",
    "### right click script and copy path (change to forward slash)\n",
    "src = str(git_dir) + str('/python/logit_python/aml_bank/logit_python_aml_bank_sagemaker.ipynb')\n",
    "dst = output_path\n",
    "shutil.copy(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/logit_python_aml_bank_sagemaker.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sagemaker-user/outputs/logit_python_aml_bank_workbench')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "### create directories for metadata\n",
    "output_path = Path('/home/sagemaker-user') / metadata_output_dir / model_name\n",
    "if output_path.exists() and output_path.is_dir():\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "### create output path\n",
    "os.makedirs(output_path)\n",
    "output_path\n",
    "\n",
    "### create python requirements file\n",
    "requirements = [\n",
    "    {\n",
    "        \"step\":\"import math, pickle, pandas as pd, numpy as np, settings\",\n",
    "        \"command\":\"pip3 install math==3.10.5 pickle==3.10.5 numpy==1.20.3 pandas==1.3.4 settings==0.2.2\"\n",
    "    }\n",
    "]\n",
    "requirementsObj = json.dumps(requirements, indent = 4)\n",
    "with open(str(output_path)+str('/requirements.json'), 'w') as outfile:\n",
    "    outfile.write(requirementsObj)\n",
    "    \n",
    "### copy .py script to output path\n",
    "### right click script and copy path (change to forward slash)\n",
    "src = '/home/sagemaker-user/logit_python_aml_bank_sagemaker.ipynb'\n",
    "dst = output_path\n",
    "shutil.copy(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logit_python_aml_bank_sagemaker was successfully pickled and saved to C:\\Users\\chparr\\OneDrive - SAS\\python\\outputs\\logit_python_aml_bank_sagemaker\\logit_python_aml_bank_sagemaker.pickle.\n",
      "inputVar.json was successfully written and saved to C:\\Users\\chparr\\OneDrive - SAS\\python\\outputs\\logit_python_aml_bank_sagemaker\\inputVar.json\n",
      "outputVar.json was successfully written and saved to C:\\Users\\chparr\\OneDrive - SAS\\python\\outputs\\logit_python_aml_bank_sagemaker\\outputVar.json\n",
      "fileMetadata.json was successfully written and saved to C:\\Users\\chparr\\OneDrive - SAS\\python\\outputs\\logit_python_aml_bank_sagemaker\\fileMetadata.json\n",
      "ModelProperties.json was successfully written and saved to C:\\Users\\chparr\\OneDrive - SAS\\python\\outputs\\logit_python_aml_bank_sagemaker\\ModelProperties.json\n"
     ]
    }
   ],
   "source": [
    "import sasctl.pzmm as pzmm\n",
    "\n",
    "input_df = X_train\n",
    "output_vars = pd.DataFrame(columns=prediction_labels, data=[[0.5, 0.5, 'A']])\n",
    "\n",
    "### create metadata\n",
    "pzmm.PickleModel.pickle_trained_model(trained_model=dm_model, model_prefix=model_name, pickle_path=output_path)\n",
    "pzmm.JSONFiles().write_var_json(input_data=input_df, is_input=True, json_path=output_path)\n",
    "pzmm.JSONFiles().write_var_json(input_data=output_vars, is_input=False, json_path=output_path)\n",
    "pzmm.JSONFiles().write_file_metadata_json(model_prefix=model_name, json_path=output_path)\n",
    "pzmm.JSONFiles().write_model_properties_json(\n",
    "    model_name=model_name, \n",
    "    target_variable=dm_dec_target,\n",
    "    target_values=event_prob_var,\n",
    "    json_path=output_path,\n",
    "    model_desc=description,\n",
    "    model_algorithm=model_type,\n",
    "    model_function=model_function,\n",
    "    modeler=username,\n",
    "    train_table=table_name,\n",
    "    properties=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chparr\\AppData\\Roaming\\Python\\Python311\\site-packages\\sasctl\\pzmm\\write_score_code.py:2118: UserWarning: No current session connection was found to a SAS Viya server. Score code will be written under the assumption that the target server is SAS Viya 4.\n",
      "  warn(\n",
      "C:\\Users\\chparr\\AppData\\Roaming\\Python\\Python311\\site-packages\\sasctl\\pzmm\\write_score_code.py:458: UserWarning: No current session connection was found to a SAS Viya server. Score code will be written under the assumption that the target server is SAS Viya 4.\n",
      "  warn(\n",
      "C:\\Users\\chparr\\AppData\\Roaming\\Python\\Python311\\site-packages\\sasctl\\pzmm\\write_score_code.py:1500: UserWarning: Due to the ambiguity of the provided metrics and prediction return types, the score code assumes that a classification and the target event probability should be returned.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'model_file_name': 'logit_python_aml_bank_sagemaker.pickle'}\n",
    "\n",
    "pzmm.ScoreCode.write_score_code(\n",
    "    model_prefix=model_name,\n",
    "    input_data=input_df,\n",
    "    predict_method=[dm_model.predict_proba, [0.4, float]],\n",
    "    target_variable=dm_dec_target,\n",
    "    target_values=event_prob_var,\n",
    "    score_metrics=None,\n",
    "    predict_threshold=None,\n",
    "    model=None,\n",
    "    pickle_type=\"pickle\",\n",
    "    missing_values=False,\n",
    "    score_cas=True,\n",
    "    score_code_path=output_path,\n",
    "    target_index=None,\n",
    "    **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BytesIO at 0x2952833c8b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pzmm.ZipModel.zip_files(        \n",
    "    model_files=output_path,\n",
    "    model_prefix=model_name,\n",
    "    is_viya4=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
